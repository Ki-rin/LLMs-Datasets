Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ufaj20 Financial Analysts Journal ISSN: (Print) (Online) Journal homepage: www.tandfonline.com/journals/ufaj20 Fundamental Analysis via Machine Learning Kai Cao & Haifeng You To cite this article: Kai Cao & Haifeng You (2024) Fundamental Analysis via Machine Learning, Financial Analysts Journal, 80:2, 74-98, DOI: 10.1080/0015198X.2024.2313692 To link to this article: https://doi.org/10.1080/0015198X.2024.2313692 View supplementary material Published online: 21 Mar 2024. Submit your article to this journal Article views: 3106 View related articles View Crossmark data Fundamental Analysis via Machine Learning Kai Cao and Haifeng You, CFA Kai Cao is an independent researcher. Haifeng You, CFA, is a chair professor of accounting in the School of Economics and Management and the Shenzhen International Graduate School at Tsinghua University. Send correspondence to Haifeng You at hyou@sem.tsinghua. edu.cn. We examine the efﬁcacy of machine learning in a central task of fundamental analysis: forecast- ing corporate earnings. We ﬁnd that machine learning models not only generate signiﬁcantly more accurate and informative out-of- sample forecasts than the state- of-the-art models in the literature but also perform better compared to analysts’ consensus forecasts. This superior performance appears attributable to the ability of machine learning to uncover new information through identify- ing economically important pre- dictors and capturing nonlinear relationships. The new informa- tion uncovered by machine learn- ing models is of considerable economic value to investors. It has signiﬁcant predictive power with respect to future stock returns, with stocks in the most favorable new information quintile outperforming those in the least favorable quintile by approxi- mately 34 to 77 bps per month on a risk-adjusted basis. Keywords: earnings forecasts; equity valuation; fundamental analysis; machine learning; market efﬁciency Disclosure: No potential conﬂict of interest was reported by the author(s). PL Credits: 2.0 Introduction Fundamental analysis is a cornerstone of capital market operations. It requires the assimilation and processing of vast and varied data, often incurring considerable costs. The primary objective of this paper is to assess the potential of machine learning (ML) to enhance a central task of fundamental analysis: the forecasting of corporate earnings (e.g., Penman 1992; Lee 1999; Richardson, Tuna, and Wysocki 2010; Monahan 2018). The literature has laid a substantial foundation with various earnings prediction models; however, their performance often does not surpass that of the rudimentary random walk (RW) approach (Easton, Kelly, and Neuhierl 2018; Monahan 2018). We posit that the shortcomings of these models, particularly their methodological con- straints, could be mitigated by leveraging ML technologies. Corporate earnings are the cumulative result of a myriad of transac- tions, each reﬂected within various ﬁnancial statement items that can have disparate impacts on future earnings. The intricate nature of these transactions, supported by both economic theory and empirical ﬁndings, suggests that the relationships between ﬁnancial statement items and subsequent earnings are often nonlinear.1 ML algorithms, with their inherent design to process high-dimensional data and dis- cern complex nonlinear and interaction effects (e.g., Gu, Kelly, and Xiu 2020), are potentially well suited to capture these nuanced effects and intricate patterns. However, the intricacy of ML models can also lead to overﬁtting. Therefore, the true efﬁcacy of ML in providing superior earnings forecasts remains an empirical question. To answer this research question, we develop an ML earnings forecast- ing model that combines three popular ML algorithms: two algorithms based on decision trees (i.e., random forest [RF] and gradient boosting regression [GBR]) and one based on artiﬁcial neural networks (ANNs). We supply these algorithms with both the level and ﬁrst-order difference Research Research funding support from Hong Kong RGC project no. T31-604/18-N is highly appreciated. 74 © 2024 CFA Institute. All rights reserved. Volume 80, Number 2 Financial Analysts Journal | A Publication of CFA Institute https://doi.org/10.1080/0015198X.2024.2313692 of a comprehensive set of ﬁnancial statement items,2 let the algorithms “learn” from the historical data to determine the underlying relationships, and generate out-of-sample forecasts for 134,154 ﬁrm-year observa- tions from 1975 to 2019. We compare the accuracy, information content, and investment value of the out- of-sample ML forecasts to the forecasts obtained from the RW model and ﬁve other models developed in the extant literature: the (ﬁrst-order) autoregressive model (AR), two models (HVZ and SO) developed by Hou, van Dijk, and Zhang (2012) and So (2013), respectively; and the earnings persistence (EP) and residual income (RI) models proposed by Li and Mohanram (2014). We ﬁnd that the ML forecasts are signiﬁcantly more accurate than not only the RW model but also all extant models. The mean absolute forecast errors of the extant models are approximately 7.79% to 26.53% greater than that of the ML model. Cross- sectional analyses indicate that the ML model leads to even greater accuracy improvements among ﬁrms with more difﬁcult-to-forecast earnings. The ML fore- cast also has greater information content as measured by its predictive power with respect to future actual earnings changes (ECH). Forecasted earnings changes (FECH) based on the ML forecast explain 18.61% of the variation in ECH, whereas FECH based on the extant models only explain about 8.07% to 12.22%. We then test whether the new information uncov- ered by the ML model can lead to signiﬁcant improvements in investment decision-making. To this end, we orthogonalize the ML forecast against the contemporaneous forecasts from the extant models and use the residuals to measure the new informa- tion uncovered by the ML forecasts (beyond the extant models). The results show that the new infor- mation component has signiﬁcant predictive power with respect to future stock returns. The top quintile of stocks with the most favorable new information signiﬁcantly outperforms the least favorable new information quintile by approximately 34 to 77 bps per month on a risk-adjusted basis. We also ﬁnd that the ML forecasts perform well against analyst consensus forecasts, even though ana- lysts have access to much more information than the ﬁnancial statements. First, the ML forecast has signiﬁ- cantly lower mean absolute forecast errors than analyst consensus earnings forecasts for all three-year forecast horizons. Second, the ML forecast has greater relative information content than analyst forecasts in predicting future earnings changes. Third, the ML forecast con- tains a signiﬁcant amount of information that analysts fail to consider even though our ML forecasts are based on ﬁnancial statement data only. Our paper contributes to the burgeoning literature on the application of ML in ﬁnance and accounting research (e.g., Chen et al. 2022; Gu, Kelly, and Xiu 2020; Bao et al. 2020; Bertomeu et al. 2021; Ding et al. 2020) by demonstrating the decisional useful- ness of ML technology in one of the most important tasks in fundamental analysis: corporate earnings forecasting. We show that by identifying economi- cally important predictors and capturing subtle non- linear relationships, ML can better utilize ﬁnancial statement information and produce signiﬁcantly more accurate and informative earnings forecasts. Our paper is also of interest to investment professio- nals. First, our model can be used to generate earn- ings forecasts at a low cost even for ﬁrms with a short history and those without analyst coverage. It not only offers a less biased alternative to analyst forecasts as a valuation input by stock pickers (e.g., Ohlson 1995; Ohlson and Juettner-Nauroth 2005) but also can be directly used to value many ﬁrms without analyst coverage. Second, investors may also easily modify our models to forecast other funda- mental variables (e.g., sales, gross proﬁt) that are of considerable importance for investors. Third, our paper also offers a potential systematic trading strat- egy to quantitative investors, who can build on our study and reﬁne the model by, for example, forecast- ing future quarterly earnings and other fundamentals or by incorporating non–ﬁnancial statement informa- tion to further improve returns to the strategy. Related Literature and Extant Earnings Forecasting Models As Monahan (2018) summarizes, early research mostly adopted the time-series approach to forecast future earnings (e.g., Ball and Watts 1972; Watts and Leftwich 1977). Overall, their results suggest that the simple RW model, which predicts expected future earnings to equal current earnings, generates more accurate out-of-sample forecasts than the more sophisticated autoregressive integrated moving aver- age (ARIMA) models (e.g., Brown 1993; Kothari 2001).3 The superiority and simplicity of the RW model make it a natural benchmark in evaluating other earnings forecasting models. There are several potential reasons for the poor per- formance associated with the ARIMA models. First, these models require a long time series to yield Fundamental Analysis via Machine Learning Volume 80, Number 2 75 reliable parameter estimates, but the earnings pro- cess may not be stationary over a long period. Second, these ﬁrm-speciﬁc time-series models ignore the rich information in other ﬁnancial statement line items. To overcome these limitations, subsequent studies turn to cross-sectional or panel-data approaches, which use a pooled cross-section of ﬁrms to estimate forecasting models. These models are considered as state-of-the-art earnings prediction models in the literature (e.g., Gerakos and Gramacy 2013; Call et al. 2016).4 We therefore employ them as alternative benchmarks. We discuss these models below and summarize them with detailed variable deﬁnitions in Appendix 1 Panel A. The ﬁrst cross-sectional model that we examine is the ﬁrst-order AR model: Ei, tþ1 ¼ a0 þ a1Ei, t þ ei, tþ1 (1) where Ei, t is ﬁrm i’s earnings in year t. Gerakos and Gramacy (2013) show that the AR model performs well relative to the RW model and is more accurate than many sophisticated models. The second model, the HVZ model, proposed by Hou, van Dijk, and Zhang (2012), extends the Fama and French (2000, 2006) model and forecasts future earnings as follows: Ei, tþ1 ¼ a0 þ a1Ai, t þ a2Di, t þ a3DDi, t þ a4Ei, t þ a5NegEi, t þ a6ACi, t þ ei, tþ1 (2) This model forecasts future earnings with total assets (Ai, t), the dividend payment (Di, t), the dividend-paying dummy (DDi, t), historical earnings (Ei, t), and an indica- tor variable for negative earnings (NegEi, t) and accruals (ACi, t). The third model, the SO model, is developed by So (2013). So modiﬁes the Fama and French (2006) model and forecasts future earnings per share (EPS) using the following model: EPSi, tþ1 ¼ a0 þ a1EPSþ i, t þ a2NegEi, t þ a3AC− i, t þ a4ACþ i, t þ a5AGi, t þ a6NDDi, t þ a7DIVi, t þ a8BTMi, t þ a9Pricei, t þ ei, tþ1 (3) where EPSþ i, t is the EPS for positive earnings and is zero otherwise; NegEi, t is an indicator variable for negative earnings; AC− i, t is accruals per share for negative accruals and is zero otherwise; ACþ i, t is accruals per share for positive accruals and is zero otherwise; AGi, t is the percentage change in total assets; NDDi, t indicates a zero dividend; DIVi, t is dividends per share; BTMi, t is the book-to-market ratio; and Pricei, t is the stock price at the end of the third month after the end of ﬁscal year t. In addition to ﬁnancial statement items, the SO model uses the stock price and book-to-market ratio, allowing it to utilize more forward-looking information. The ﬁnal two cross-sectional models are proposed by Li and Mohanram (2014): Ei, tþ1 ¼ a0 þ a1NegEi, t þ a2Ei, t þ a3NegEi, t � Ei, t þ ei, tþ1 (4) Ei, tþ1 ¼ a0 þ a1NegEi, t þ a2Ei, t þ a3NegEi, t � Ei, t þ a4BVEi, t þ a5TACCi, t þ ei, tþ1 (5) Equation (4) allows loss ﬁrms to have different earnings persistence (the EP model) from proﬁtable ﬁrms. Equation (5) is based on the residual income model (the RI model) proposed by Feltham and Ohlson (1996), which further augments the EP model with the book value of equity (BVEi, t) and total accruals (TACCi, t) from Richardson et al. (2005). Although the above models are the state-of-the-art models in the literature, they fail to consistently out- perform the RW model (e.g., Monahan 2018; Easton, Kelly, and Neuhierl 2018). Taken at face value, these results seem to suggest that there is not much incre- mental information in ﬁnancial statements beyond the bottom-line earnings, which contradicts both the conventional wisdom and the fundamental tenet of ﬁnancial statement analysis. Given that “the question of whether historical accounting numbers are useful for forecasting earnings is central to accounting research” (p. 183, Monahan 2018), both Monahan (2018) and Easton, Kelly, and Neuhierl (2018) call for further research in this area. We posit that, rather than indi- cating the lack of the decisional usefulness of ﬁnan- cial statements or fundamental analysis, the above results might be driven by the methodological limita- tions of the extant models, which can be overcome by ML algorithms. ML-Based Earnings Forecasts The extant models do not make the best use of information in ﬁnancial statements to forecast future earnings. First, the extant models focus on a small number of aggregate ﬁnancial statement items, such as bottom-line earnings and total assets, and fail to fully consider many other ﬁnancial statement line Financial Analysts Journal | A Publication of CFA Institute 76 items that could be highly valuable for earnings pre- diction (e.g., Fairﬁeld, Sweeney, and Yohn 1996; Chen, Miao, and Shevlin 2015). Second, even though economic theories and empirical evidence suggest the prevalence of nonlinear relationships between historical accounting information and future earnings (e.g., Lev 1983; Freeman and Tse 1992; Baginski et al. 1999; Chen and Zhang 2007), these models mostly adopt linear functional forms (or with some simple interactions) and are therefore unlikely to capture these subtle yet important rela- tionships. ML algorithms are designed to handle high-dimensional data and are rather ﬂexible with respect to the functional forms of the underlying relationships. Thus, they can potentially overcome the above limitations and generate better earnings forecasts. Below we describe the development of our ML models. Financial Statement Line Items as Predictors. Both the ﬁnancial ratios and raw val- ues of ﬁnancial statement items can be used in forecasting future earnings.5 A priori, it is unclear which choice is better. On the one hand, ﬁnancial ratios are grounded in economic theories, which may help ﬁlter out the noise in the raw values of ﬁnancial items and better capture the key drivers of future earnings. On the other hand, the transforma- tion from raw values to ratios may cause a loss or distortion of information. In a recent study, Bao et al. (2020) compare the predictive power of ML models based on raw accounting numbers versus those based on ﬁnancial ratios with respect to accounting fraud. They ﬁnd that raw value–based models perform signiﬁcantly better. Following their study, we rely on raw accounting values as our pri- mary set of predictors and examine alternative pre- dictor sets of ﬁnancial ratios in additional analyses. Furthermore, we follow prior literature (e.g., Li and Mohanram 2014) and scale all raw accounting val- ues (including both the input features and the tar- get variable) by the number of common shares outstanding. As Richardson, Tuna, and Wysocki (2010) suggest, a concern with the early literature of fundamental anal- ysis is the in-sample identiﬁcation of predictive varia- bles. To mitigate this concern, we select a comprehensive list of key ﬁnancial statement items and let ML algorithms “learn” from historical data in terms of how to optimally select and combine these items. The resulting models are then used to gener- ate out-of-sample earnings predictions. The detailed list of ﬁnancial statement items and the rationale for their inclusion are provided in Appendix 1 Panel B. The input predictors (a total of 60) can be broadly categorized as follows: i. Historical earnings and their major compo- nents (8). We include earnings components, as prior literature has shown that disaggregat- ing earnings provides additional information and improves earnings forecasts (Fairﬁeld, Sweeney, and Yohn 1996; Chen, Miao, and Shevlin 2015). ii. Other important individual income statement items (5). Speciﬁcally, we include advertising and R&D expenses, as they tend to generate long-term future beneﬁts (e.g., Lev and Sougiannis 1996; Dou et al. 2021). We also include special and extraordinary items and dis- continued operations, as ﬁrms may shift core expenses to these items to manipulate core earnings (e.g., Barnea, Ronen, and Sadan 1976; McVay 2006). Finally, we include common divi- dends, as they may signal ﬁrms’ future earning power (e.g., Nissim and Ziv 2001). iii. Summary and individual balance sheet items (16). The balance sheet summarizes resources with potential future economic beneﬁts and may contain incremental information. For exam- ple, the literature shows that the book value of equity is an important driver of future earnings (e.g., Feltham and Ohlson 1995; Li and Mohanram 2014), and Ball et al. (2020) argue that retained earnings may measure average earning power better than the book value of equity. iv. Operating cash ﬂows (1). We include cash ﬂows from operating activities, as prior studies have found that the cash ﬂow component of earn- ings is more persistent than the accrual compo- nent, and separating them helps predict future earnings (e.g., Sloan 1996; Call et al. 2016). v. The ﬁrst-order differences of the above items (30). We include them, as prior studies show that changes in ﬁnancial statement items often contain incremental information beyond the levels (e.g., Kothari 1992; Ohlson and Shroff 1992; Richardson et al. 2005). ML Algorithms. Following prior studies (e.g., Rasekhschaffe and Jones 2019; Cao et al. 2024), we develop a ML earnings prediction model by ensembling the out-of-sample earnings forecasts generated from several popular ML algorithms. Among them, two algorithms are based on decision trees and one is based on ANNs. Our two decision Fundamental Analysis via Machine Learning Volume 80, Number 2 77 tree–based algorithms are the standard RF and the GBR algorithms. In implementing our ANN-based algorithm, we adopt the bootstrap aggregating (i.e., bagging) technique by constructing 10 bootstrap samples, with each sample randomly drawing 60% of the observations from the training set. Thereafter, we train an ANN model for each boot- strapped sample and then average the 10 models to generate predictions.6 Cross-Validation and Hyperparameter Tuning. In the implementation of ML, it is impera- tive to select a model with an appropriate level of complexity because overly simple models tend to underﬁt the data, while overly complex models tend to have an overﬁtting problem, and both lead to poor out-of-sample predictability. The level of model com- plexity is largely determined by the value of certain hyperparameters, which must be set before estimat- ing other parameters such as regression coefﬁcients and neural network weights. For example, values of the hyperparameters such as the maximum depth of the decision trees in the RF and GBR models deter- mine the overall model complexity as well as the number/fraction of input features effectively used in the model. We search for the “optimal” hyperparameter values through hyperparameter tuning via cross-validation. We use ﬁvefold cross-validation to identify the opti- mal hyperparameter values that generate the most accurate forecasts on the validation samples. Speciﬁcally, for each year in our test sample, we use data from the previous 10 years to train the ML mod- els. We randomly split the 10 years of data into ﬁve groups/folds of validation sets, with each fold includ- ing 20% of the data. For each fold, we use the remaining 80% of the ﬁrm-year observations as the training sample. For each of the ML algorithms, we provide a set of reasonable candidate values for the key hyperparameters (see details provided in Appendix 2). For each combination of the hyperpara- meters, we compute the mean squared forecast errors of the ﬁve validation sets using the model esti- mated from the remaining 80%, which forms the training set. The mean squared forecast errors on the validation sets are used as the basis to select the optimal hyperparameters, which are then used to train a new model on the training set. We then apply the model to the current year’s ﬁnancial statement data to generate out-of-sample earnings forecasts for the following year, which are then compared to the subsequent actual earnings to evaluate the relative performance of various models. Data, Sample Selection, and Model Estimation Procedure Our initial sample comprises 267,777 ﬁrm-year observations obtained from the intersection of the Compustat fundamentals annual ﬁle7 and the Center for Research in Security Prices (CRSP) data up to ﬁs- cal year 2019. We further impose the following data requirements: (1) the following ﬁnancial statement items must be non-missing: total assets, sales reve- nue, income before extraordinary items, and common shares outstanding; (2) the stocks must be ordinary common shares listed on the NYSE, AMEX, or NASDAQ; (3) the ﬁrms cannot be in the ﬁnancial (SIC 6000–6999) or regulated utilities (SIC 4900–4999) industries; and (4) the stock prices at the end of the third month after the end of the ﬁscal year must be greater than US$1. Among the remaining ﬁrm-year observations, we replace missing values of the cash ﬂow from operating activities with the corresponding numbers computed from the balance sheet approach (Sloan 1996). We then set the missing values of the remaining line items to zero before computing the ﬁrst-order differences of the 30 items in Appendix 1. This leaves us with a ﬁnal sample of 156,256 obser- vations from 1965 to 2019. Because we need data from the past 10 years to estimate the models, our testing sample (i.e., prediction set) starts from 1975 and consists of 142,592 ﬁrm-year observations. Table 1 presents the number of ﬁrms in the ﬁnal testing sample by year, where the number of annual observations ranges from 2,299 in 2019 to 4,976 in 1997. At the third month end after ﬁscal year t, we gener- ate the out-of-sample forecasts of one-year-forward earnings Etþ1 for the above testing sample using the aforementioned machine learning algorithms and the 60 predictors. Following prior literature (e.g., Hou, van Dijk, and Zhang 2012; Li and Mohanram 2014), for each year t between 1975 and 2019, we use all observations from the previous 10 years (i.e., year t − 10, t − 9, … , t − 1) to estimate the models. As dis- cussed earlier, we use ﬁvefold cross-validation to identify the optimal hyperparameters. Using these optimal values, we retrain the model using the obser- vations from the previous 10 years and then apply the trained models to the predictors of year t to gen- erate earnings forecasts for year t þ 1. For consis- tency, all extant models are also estimated using the data of the same previous 10 years, and the resulting linear models are applied to their respective predic- tors in year t to generate earnings forecasts for year t þ 1.8 Financial Analysts Journal | A Publication of CFA Institute 78 Empirical Results Comparison of Forecast Accuracy. To eval- uate the forecast accuracy of the different models, we compare the mean and median absolute forecast errors. We deﬁne the forecast error as the difference between the predicted and actual earnings deﬂated by the market value of equity at the end of three months after the ﬁscal year end. A larger absolute forecast error indicates less accurate earnings fore- casts.9 Table 2 reports the time-series average of the out-of-sample annual mean and median absolute forecast errors of all models. The ML forecast turns out to be the most accurate forecast, with an aver- age mean absolute forecast error of 0.0687 and an average median absolute forecast error of 0.0291. The benchmark RW model, which prior literature shows to be very difﬁcult to outperform, has an aver- age mean (median) absolute forecast error of 0.0764 (0.0309), approximately 11.20% (6.12%) higher than that of the ML forecast. Consistent with the litera- ture, the extant models are not reliably more accu- rate than the naïve RW model. More importantly, all extant forecasts are less accurate than the ML fore- casts, with mean (median) absolute forecast errors approximately 7.79% to 26.53% (5.87% to 19.45%) higher than that of the ML forecast. Panels B and C of Table 2 present the results for two- and three- year-ahead out-of-sample earnings forecasts, respec- tively. The results show that ML remains the most accurate forecast. For example, the mean (median) absolute forecast error of RW is approximately 13.06% (6.93%) and 12.54% (6.16%) higher than that of the ML model for two- and three-year-ahead fore- casts, respectively. We also examine whether the ML model predicts the rank and relative level of earnings better than the extant models. For rank prediction, we ﬁrst rank both the actual earnings and earnings forecasts cross-sectionally and normalize the ranks to [0,1] with the following transformation: RANKnorm ¼ (RANK − RANKmin)/(RANKmax − RANKmin). We then compute the mean absolute difference in the normal- ized ranks between the actual earnings and the earn- ings forecast. The results presented in the ﬁrst ﬁve columns of Table 2 Panel D show that the ML fore- cast still has the lowest mean absolute error of 0.1007, which is about 5% to 14.5% lower than that of the extant model forecasts. For the accuracy of forecasting the relative level, we ﬁrst remove from both the actual and forecasted earnings their respec- tive cross-sectional median and then compute the mean squared error (MSE) of the median-adjusted numbers. The results presented in the last four col- umns of Table 2 Panel D show that the ML forecast again has the highest accuracy, with the MSE being about 16.5% to 56.9% lower than that of the extant models. Cross-Sectional Analysis. The above results suggest that the ML models generate signiﬁcantly more accurate earnings forecasts than the RW model. For ﬁrms with stable performance, histori- cal earnings are quite good indicators for future earnings. The beneﬁt of considering additional ﬁnancial statement line items and more complex forms of relationships should be of higher impor- tance for ﬁrms with more difﬁcult-to-forecast earnings. Table 3 reports the percentage improve- ment in the forecast accuracy of the ML model rel- ative to the benchmark RW model for subgroups partitioned along the following dimensions: Return on Assets (ROA) volatility, magnitude of accruals, R&D expense, and an indicator variable of loss ﬁrms. The results in Panel A of Table 3 convey the follow- ing key messages: ﬁrst, the ML forecast is signiﬁ- cantly more accurate than the RW forecast across the board for all subgroups. Second, ML models lead to signiﬁcantly greater accuracy improvement among ﬁrms with more difﬁcult-to-forecast earnings. For example, relative to the RW model, the ML forecast leads to an accuracy improvement of 15.42%, 17.80%, 11.46%, and 12.99% among ﬁrms in the Table 1. Sample Distribution by Year Year # obs Year # obs Year # obs 1975 2,550 1990 3,029 2005 3,303 1976 2,558 1991 3,140 2006 3,259 1977 2,578 1992 3,484 2007 3,166 1978 2,593 1993 3,816 2008 2,945 1979 2,679 1994 4,236 2009 2,583 1980 2,694 1995 4,373 2010 2,747 1981 2,685 1996 4,690 2011 2,673 1982 2,689 1997 4,976 2012 2,538 1983 2,830 1998 4,930 2013 2,499 1984 2,892 1999 4,620 2014 2,522 1985 3,047 2000 4,540 2015 2,497 1986 3,087 2001 3,969 2016 2,419 1987 3,083 2002 3,595 2017 2,383 1988 3,219 2003 3,310 2018 2,349 1989 3,140 2004 3,378 2019 2,299 Notes: This table reports the number of ﬁrms with non-missing input features for all models from 1975 to 2019. Fundamental Analysis via Machine Learning Volume 80, Number 2 79 highest quintiles of the ROA volatility, magnitude of total accruals, R&D expense, and loss makers, respec- tively. In Panel B of Table 3, we conduct a regression analysis of the difference in the forecast accuracy between RW and ML forecasts on the above deter- minants. Columns (1) through (5) report the univariate regression results, conﬁrming the results in Panel A that ML brings greater improvement in accu- racy among ﬁrms with higher earnings volatility, a larger magnitude of accruals, more R&D expense, and loss makers. The multivariate regression results in column (6) are largely consistent, except that the Table 2. Comparison of Forecast Accuracy A: Accuracy of one-year-ahead earnings forecasts (N = 134,154 firm-years) Mean Median Average*100 Comparison with ML Average*100 Comparison with ML DIFF t stat. %DIFF DIFF t stat. %DIFF ML 6.87 2.91 RW 7.64 0.77 7.85 11.20% 3.09 0.18 5.73 6.12% AR 7.55 0.68 8.96 9.93% 3.08 0.17 6.16 5.87% HVZ 7.43 0.55 8.31 8.07% 3.11 0.20 7.95 6.93% EP 7.42 0.55 8.06 8.03% 3.13 0.22 7.89 7.63% RI 7.41 0.54 7.61 7.79% 3.11 0.20 7.82 6.90% SO 8.70 1.82 12.72 26.53% 3.47 0.57 14.81 19.45% B: Accuracy of two-year-ahead earnings forecasts (N = 123,576 firm-years) Mean Median Average*100 Comparison with ML Average*100 Comparison with ML DIFF t stat. %DIFF DIFF t stat. %DIFF ML 9.09 4.42 RW 10.28 1.19 8.07 13.06% 4.73 0.31 5.25 6.93% AR 10.18 1.09 9.50 11.99% 4.70 0.28 9.76 6.41% HVZ 9.71 0.62 7.71 6.84% 4.62 0.19 6.68 4.41% EP 9.64 0.55 6.03 6.06% 4.66 0.23 6.56 5.31% RI 9.56 0.47 5.29 5.22% 4.60 0.18 6.01 4.14% SO 10.31 1.22 11.87 13.42% 4.91 0.49 12.80 11.01% C: Accuracy of three-year-ahead earnings forecasts (N = 113,601 firm-years) Mean Median Average*100 Comparison with ML Average*100 Comparison with ML DIFF t stat. %DIFF DIFF t stat. %DIFF ML 10.88 5.58 RW 12.25 1.37 7.28 12.54% 5.92 0.34 3.38 6.16% AR 12.27 1.38 9.86 12.69% 5.93 0.35 8.02 6.28% HVZ 11.42 0.53 6.96 4.88% 5.73 0.15 4.07 2.64% EP 11.38 0.50 5.04 4.56% 5.79 0.21 5.03 3.73% RI 11.21 0.33 3.73 3.03% 5.70 0.12 3.37 2.20% SO 12.03 1.14 9.62 10.50% 6.11 0.53 11.41 9.47% continued Financial Analysts Journal | A Publication of CFA Institute 80 Table 3. Cross-Sectional Analysis of Improvement in Forecast Accuracy A: The percentage improvement in accuracy of the ML forecast relative to the RW forecasts, i.e., (|RW forecast errors| − |ML forecast errors|)/|RW forecast errors| *100% (N = 129,310, with an average of 2,874 firms per year over the 45- year sample period from 1975 to 2019) Partitioning variable Low 2 3 4 High ROA Volatility 4.86 5.77 5.88 9.15 15.42 |Total Accruals|/Total Assets 3.51 4.45 6.37 10.12 17.80 |Working Capital Accruals|/Total Assets 6.67 7.67 7.81 8.68 13.81 MISSING Low 2 3 High R&D Expense/Total Assets 9.11 8.54 10.24 10.78 11.46 Non-Loss Loss Loss 6.64 12.99 B: Multivariate regression analysis of the improvement in the accuracy of ML forecasts relative to the RW forecasts (N = 129,310) Y = |RW forecast errors| − |ML forecast errors| (1) (2) (3) (4) (5) (6) ROA Volatility 0.050 0.006 (6.25) (1.45) |Total Accruals|/Total Assets 0.055 0.038 (7.35) (6.44) |Working Capital Accruals|/Total Assets 0.095 0.053 (7.92) (7.32) R&D Expense/Total Assets 0.012 −0.037 (2.73) (−4.74) Loss 0.020 0.016 (6.39) (5.29) continued Table 2. (continued) D: Alternative measures of forecast accuracy (N = 134,154 firm-years) MAE of Ranks MSE of De-medianed Forecasts Average*100 Comparison with ML Average*100 Comparison with ML DIFF t stat. %DIFF DIFF t stat. %DIFF ML 10.07 1.76 RW 10.58 0.51 10.92 5.03% 2.21 0.45 3.52 25.67% AR 10.63 0.56 12.46 5.55% 2.13 0.37 3.52 20.87% HVZ 10.71 0.63 15.04 6.30% 2.06 0.29 3.32 16.66% EP 10.58 0.51 10.95 5.04% 2.07 0.31 3.72 17.51% RI 10.63 0.56 12.26 5.53% 2.05 0.29 3.34 16.54% SO 11.54 1.47 11.79 14.54% 2.77 1.00 8.77 56.90% Notes: This table compares the accuracy between the machine learning (ML) forecast and the extant models over the sample period of 1975–2019. Panels A, B, and C report the time-series average of the mean and median absolute forecast errors of one-, two-, and three-year-ahead earnings forecasts, respectively. The absolute forecast error is calculated as the absolute value of the difference between the actual future earnings and the earnings forecasts, scaled by the market equity at the end of three months after the end of the last ﬁscal year. Panel D reports alternative measures of forecast accuracy, i.e., the time-series average of the mean absolute errors of the scaled rank of forecasts difference and the mean squared errors of the de-medianed forecasts differ- ence. DIFF is the time-series average of the difference, calculated as the mean (median) absolute forecast error of each model minus that of the ML model. The t statistic of DIFF time-series is reported accordingly. The percentage difference (%DIFF) is DIFF divided by the time-series average of the annual mean (median) absolute forecast error of the ML model. Fundamental Analysis via Machine Learning Volume 80, Number 2 81 ROA volatility becomes statistically insigniﬁcant and the coefﬁcient on R&D expense ﬂips its sign in the presence of other determinants. Information Content Analysis. Forecast accu- racy is not the sole determinant of the decision use- fulness of earnings forecasts. For example, although the RW forecast is more accurate than other fore- casts, it provides no information with respect to future earnings changes. In this section, we evaluate the information content of various models by investi- gating their (out-of-sample) predictive power with respect to the future earnings change, ECH. ECH is computed as the difference between earnings in year t þ 1 and those in year t, scaled by market capitaliza- tion at the end of the third month after the end of ﬁscal year t. We calculate the forecasted earnings change, or FECH, as the predicted earnings for year t þ 1 minus the actual earnings for year t, scaled by market capitalization at the third month end after the end of ﬁscal year t. We ﬁrst compare the mean correlation coefﬁcients between the ECH and FECH calculated from differ- ent models. The left two panels of Table 4 show that the Pearson (Spearman) correlation coefﬁcients between FECH and ECH range from 0.199 to 0.321 (0.117 to 0.179) for the extant models, which are all lower than the correlations between the ECH and FECH of the ML forecast, 0.413 (0.3). We then esti- mate the univariate Fama–MacBeth regression of ECH on the FECH of different models. To facilitate the comparison of the coefﬁcients, we follow the lit- erature to standardize FECH so that it has a zero mean and unit variance each year. The three columns in the middle panel of Table 4 show that the coefﬁ- cients on FECH for the extant models range from 0.0304 to 0.0480, explaining between 8.07% and 12.22% of the cross-sectional variation in realized earnings changes. In contrast, the FECH based on the ML forecast has a regression coefﬁcient of 0.0606 and an explanatory power of 18.61%. Next, we estimate a multivariate regression of ECH on FECH based on the ML model by controlling for the FECH of all extant models. The right panel of Table 4 shows that the coefﬁcient on FECHML is signiﬁcantly positive, with a t statistic of 17.98. In contrast, most of the FECH coefﬁcients based on the extant models become statistically insigniﬁcant (or have the wrong sign), except for that of the SO model, which also uses forward-looking nonﬁnancial statement predic- tors such as the stock price and book-to-market ratio. Economic Significance Analysis. The above results suggest that ML technology helps generate more accurate earnings forecasts of (statistically) sig- niﬁcant incremental information beyond the extant models. However, it is unclear whether such new information is economically signiﬁcant. To shed light on the economic signiﬁcance of the results, we test Table 3. (continued) B: Multivariate regression analysis of the improvement in the accuracy of ML forecasts relative to the RW forecasts (N = 129,310) Y = |RW forecast errors| − |ML forecast errors| (1) (2) (3) (4) (5) (6) Const 0.005 0.002 0.001 0.007 0.003 −0.003 (6.90) (3.51) (1.01) (7.36) (5.70) (−3.50) # years 45 45 45 45 45 45 Avg. # ﬁrms per year 2,874 2,874 2,874 2,874 2,874 2,874 Avg adj. R2 0.89% 2.07% 1.83% 0.13% 2.81% 5.12% Notes: This table presents a cross-sectional analysis of the improvement in the forecast accuracy of the machine learning (ML) model, relative to that of the random walk (RW) model. The percentage improvement in Panel A is deﬁned as the time-series aver- age of the annual difference in the mean absolute forecast errors (MAFE) between the ML and RW models, divided by the MAFE of the RW model. A positive number indicates improved accuracy of the ML model. In Panel A, we sort all ﬁrms into quintiles for each year based on the magnitude of the partition variable (ROA volatility, absolute value of total accruals divided by total assets, and absolute value of working capital accruals divided by total assets, respectively). For R&D expense, we classify all ﬁrms with missing R&D expense into a separate group and sort the remaining ﬁrms into quartiles for each year based on their R&D expense divided by total assets. We also divide all ﬁrms into two groups for each year, depending on whether their earnings are negative. In Panel B, we regress the difference in the forecast accuracy on the determinants each year and report the mean coefﬁcients of the annual regressions, as well as the corresponding Fama–MacBeth t statistics. Financial Analysts Journal | A Publication of CFA Institute 82 whether the new information in the ML forecasts has (statistically and economically) signiﬁcant predictive power with respect to future stock returns.10 To cap- ture the new information uncovered by ML models, we orthogonalize the ML-based forecasts against the forecasts generated using the RW and extant models. Speciﬁcally, we run an annual cross-sectional regres- sion of the ML forecasts on the RW forecasts and the forecasts of the ﬁve extant models each year and use the residual to measure the new information uncovered by the ML models. Then, we estimate the following models to test whether the residual fore- casts predict future stock returns: EXRET12Mi, tþ1 ¼ b0 þ b1ML RESDi, t þ XS s¼1 csXi, s, t þ IndustryFE þ ei, tþ1 (6) where EXRET12Mi, tþ1 is the one-year-ahead cumula- tive excess return starting from the fourth month of ﬁscal year t þ 1 for ﬁrm i. ML RESDi, t is the residual from the regression that orthogonalizes the ML fore- cast against the RW model and the ﬁve extant mod- els. Xi, s, t is the end-of-year t value of ﬁrm i’s control characteristics. We follow Bartram and Grinblatt (2018) to select the list of control variables and also control for industry ﬁxed effects. Furthermore, we add ROE (return on equity) and INV (growth rate of total assets) in light of Fama and French (2015). The timeline for the computation of EXRET12Mi, tþ1 and ML RESDi, t is provided in Figure 1, and the detailed variable deﬁnitions are provided in Panel C of Appendix 1. If the new information component has already been fully priced (when the year t ﬁnancial statements are announced), the coefﬁcient on ML RESDi, t, i.e., b1 would be statistically insigniﬁcant. Table 5 presents the Fama–MacBeth regression results of model (6). Column (1) reports the regres- sion results using the same set of control variables as in Bartram and Grinblatt (2018). The results show that the new information component of the ML fore- cast, ML_RESD, is signiﬁcantly associated with future 12-month excess returns, even after controlling for various return-predictive factors. In column (2), we further control for ROE and INV (Fama and French 2015). The results remain robust, showing that the new information component still exhibits a positive and statistically signiﬁcant association with future stock returns.11 We also conduct a portfolio analysis. Speciﬁcally, at the beginning of each month, we estimate the new infor- mation component as the residual from the regression of the ML forecasts on the contemporaneous forecasts generated from the RW model and the ﬁve extant Table 4. Information Content Analysis Correlation with ECH Univariate regression Multivariate regression Depvar: ECH Depvar: ECH Pearson Spearman Coeff. t stat. Avg. R2 Coeff. t stat. FECHML 0.413 0.300 0.0606 12.01 18.61% 0.0589 17.98 FECHAR 0.199 0.117 0.0304 4.81 8.07% 0.0078 1.59 FECHHVZ 0.283 0.179 0.0422 8.93 9.98% −0.009 −2.48 FECHEP 0.321 0.154 0.0480 9.96 12.22% 0.0067 0.52 FECHRI 0.313 0.148 0.0467 9.95 11.68% −0.0159 −1.54 FECHSO 0.291 0.153 0.0440 10.47 9.66% 0.0132 4.46 Avg. R2 20.87% Notes: This table performs the information content analysis of the machine learning (ML) forecast against the extant models. The left panel reports the average annual cross-sectional Pearson (Spearman) correlation coefﬁcients between the forecasted earnings changes calculated using various models and the actual earnings changes for a total of 134,154 ﬁrm-year observations, with an average of 2,981 ﬁrms per year over the 45-year sample period from 1975 to 2019. The middle panel reports the univariate Fama–MacBeth regression results. In the regression, all forecasted earnings changes are standardized to have a zero mean and unit variance each year. The right panel reports the multivariate Fama–MacBeth regression results. Speciﬁcally, we regress earn- ings changes (ECH) on the forecasted earnings changes (FECH) of the ML forecasts and control for all earnings changes predicted using the extant models. All independent variables are standardized to have a zero mean and unit variance each year. All earnings changes are scaled by the market equity at the end of three months after the end of the last ﬁscal year. The table presents the average coefﬁcients, along with the Fama–MacBeth t statistics and the average adjusted R2. The subscripts are omitted for brevity. Fundamental Analysis via Machine Learning Volume 80, Number 2 83 models. We then sort all stocks into quintiles based on the residuals for each three-digit SIC industry. We con- struct a hedge portfolio that takes long positions in quintiles with the most favorable new information and short positions in quintiles with the least favorable new information. Table 6 reports the mean monthly return, Sharpe ratio (annualized), CAPM alpha, Fama–French three-factor alpha, Carhart four-factor alpha, Fama– French ﬁve-factor alpha (i.e., the three-factor model plus the Conservative Minus Aggressive (CMA) and Robust Minus Weak (RMW) factors), and the alpha after controlling for all factors in the Fama and French dataset, for the ﬁve new information component quin- tiles, as well as the hedge portfolios that take long positions in the top quintiles and short positions in the bottom quintiles. Panel A of Table 6 reports the results for the equally weighted portfolios, showing that the mean monthly excess returns increase monotonically from 0.50% for the lowest quintile to 1.23% for the highest quin- tile. The hedge portfolio generates a monthly mean return of 0.73%. The Sharpe ratio also increases monotonically from 0.25 to 0.69 for the ﬁve quin- tiles. The hedge portfolio generates a Sharpe ratio of 1.29. Furthermore, we ﬁnd that the risk-adjusted returns (or alphas) increase consistently with the quintile rank. Finally, even after controlling for all fac- tors in the Fama and French dataset, the hedge port- folio still earns a monthly alpha of 51 bps. The results for the value-weighted portfolios appear- ing in Panel B of Table 6 are slightly weaker but still signiﬁcant both statistically and economically. The hedge portfolio generates a monthly mean return of 0.48%, with a Sharpe ratio of 0.6. Furthermore, all Fama–French factor alphas are still signiﬁcant for the hedge portfolio. For example, the hedge portfolio yields a Fama–French ﬁve-factor alpha of approxi- mately 45 bps per month. In Figure 2, we plot the cumulative log returns (value-weighted) for the ﬁve quintiles and the hedge portfolio. The plot shows that the cumulative returns increase monotonically with the quintile rank, and the hedge portfolio returns are reasonably consistent over time.12 Additional Analyses The Importance of Nonlinear Effects. We conduct several additional analyses to better under- stand the underlying reasons for the superior perfor- mance of the ML forecasts. We ﬁrst plot the feature (Gini) importance charts of the RF model to check whether they use economically sensible features to generate predictions. Figure 3 shows that past earn- ings and operating cash ﬂows are important predic- tors for future earnings, ranked ﬁrst and third, respectively. Interestingly, total income tax and its ﬁrst-order difference are the second and fourth most important features, respectively. These ﬁndings are consistent with recent literature on the important role of tax income or expenses in capturing the qual- ity of earnings and predicting future fundamentals and stock returns (e.g., Lev and Nissim 2004; Hanlon 2005; Hanlon, Laplante, and Shevlin 2005; Thomas and Zhang 2011, 2014). Panels A through C of Figure 4 present the accumulated local effects (ALE) plots (Apley and Zhu 2020) of the top ﬁve most important features of the RF models13 for 1975, 1995, and 2015, respectively. The ﬁgures show obvi- ous nonlinear relationships between these input fea- tures and future earnings.14 Figure 1. Timeline of Return Prediction Analysis Notes: This ﬁgure presents the timeline for the variables used in the future return prediction analysis. Assume that ﬁscal year t þ 1 (2011) of ﬁrm i ends on 12/31/2011. In the future return prediction analysis, we regress EXRET12Mi, tþ1, which is the excess cumu- lative return over the period of 04/01/2011 to 03/31/2012 on ML RESDi, t, which is a proxy for the new information component of a machine learning forecast, EMODEL t Xi, tþ1 ½ �, estimated on 03/31/2011. Financial Analysts Journal | A Publication of CFA Institute 84 In order to provide further empirical evidence on the importance of nonlinear and interaction effects, we develop a linear forecast (LR) using the same set of input features and combine the out-of-sample predic- tions of the OLS, LASSO, and Ridge algorithms. Untabulated results suggest that on a standalone basis, the LR forecast is statistically more accurate and informative (about future ECH) than the extant models but is signiﬁcantly less accurate and informa- tive than the ML forecast. Furthermore, Panel A of Table 7 shows that in the multivariate regression of ECH on FECH based on both the LR and ML fore- casts, both the coefﬁcients and the t statistics are greater for the ML forecast than the linear forecasts. Finally, to better understand the collective impor- tance of nonlinear effects in the ML forecast, we regress the ML forecast on the 60 inputs using OLS to ﬁlter out the linear effects. The ﬁtted value of the regression (FITTED) captures the linear effect of the predictors, while the residuals capture the nonlinear and interaction effects in the ML forecasts (denoted as NONLR). We then decompose FECHML into FECHFITTED (¼FITTED − current earnings) and NONLR and examine their predictive information content. The results reported in Panel B of Table 7 show that NONLR has signiﬁcant predictive power with respect to ECH in the presence of FECHFITTED and the FECHs of the extant models, again suggest- ing that the ability to accommodate nonlinearity and interaction effects allows the ML forecast to uncover a signiﬁcant amount of new information. Comparison between ML Forecasts and Analyst Consensus Forecasts. We compare the ML forecasts to the consensus analyst forecasts (Analyst) issued around the same time, that is, the third month after the ﬁscal year end. Because Bradshaw et al. (2012) ﬁnd that the superiority of analyst forecasts over RW varies over forecast hori- zons, we conduct a comparison for horizons ranging from one to three years. Table 8 Panel A shows that the mean absolute forecast errors of the ML forecast are signiﬁcantly lower than those of the analyst con- sensus forecasts for all three forecast horizons (0.0541, 0.0679, and 0.0776 for ML vs. 0.0588, 0.0742, and 0.0922 for analyst forecasts). The median results are largely similar, except for the one- year horizon, where the ML forecast has a slightly higher median absolute forecast error (0.0219 vs. 0.0202). Panel B of Table 8 further shows that the ML forecast not only has slightly greater relative information content than analyst forecasts (see Table 5. Regression Analysis of Future 12- Month Cumulative Excess Returns on the New Information Component of Machine Learning Forecasts Dep ¼ EXRET12M (1) (2) ML_RESD 0.807 0.789 (5.55) (5.28) Beta −0.005 −0.006 (−1.08) (−1.28) SIZE 0.063 0.061 (5.87) (5.66) BM −0.044 −0.041 (−2.35) (−2.29) MOM −0.246 −0.216 (−5.09) (−4.57) ACC 0.006 0.007 (0.79) (0.94) ST_Reversal −0.064 −0.067 (−1.37) (−1.43) LT_Reversal −0.013 −0.011 (−2.58) (−2.16) SUE 0.113 0.117 (2.62) (2.64) Gross Proﬁtability 0.130 0.113 (7.55) (6.41) Earnings yield 0.208 0.174 (5.39) (5.13) ROE 0.042 (2.85) INV −0.042 (−3.07) Const YES YES Industry FE YES YES Number of years 44 44 Avg # ﬁrms per year 2,009 2,009 Avg. R2 5.59% 4.32% Notes: This table reports the Fama–MacBeth regression results that regress future one-year-ahead